\begin{table}[H]
\centering
\begin{tabularx}{0.48\textwidth}{|l|X|}
\hline
Hyperparameter & Search Space \\
\hline
\texttt{criterion} & \{gini, entropy, log\_loss\} \\
\texttt{splitter} & \{best\} \\
\texttt{max\_depth} & \{10, 100, None\} \\
\texttt{min\_impurity\_decrease} & \{0.01, 0.0001, 1e-06, 1e-08, 0\} \\
\texttt{class\_weight} & \{balanced\} \\
\hline
\end{tabularx}
\caption{Decision Trees hyperparameter search space for the first search.}
\label{tab:hyperparameters_decision_tree_0}

\end{table}


\begin{table}[H]
\centering
\begin{tabularx}{0.48\textwidth}{|l|X|}
\hline
Hyperparameter & Search Space \\
\hline
\texttt{n\_estimators} & \{10, 50, 100\} \\
\texttt{criterion} & \{gini, entropy, log\_loss\} \\
\texttt{max\_depth} & \{10, 100, None\} \\
\texttt{min\_impurity\_decrease} & \{0.01, 0.0001, 1e-06, 0\} \\
\texttt{n\_jobs} & \{-1\} \\
\texttt{class\_weight} & \{balanced\} \\
\hline
\end{tabularx}
\caption{Random Forest hyperparameter search space for the first search.}
\label{tab:hyperparameters_random_forest_0}

\end{table}


\begin{table}[H]
\centering
\begin{tabularx}{0.48\textwidth}{|l|X|}
\hline
Hyperparameter & Search Space \\
\hline
\texttt{network} & \{ff\_tfidf, lstm\_embeddings\} \\
\texttt{base\_size} & \{8, 16, 32\} \\
\texttt{depth} & \{1, 2, 3, 4\} \\
\texttt{epochs} & \{10, 15\} \\
\texttt{dropout} & \{0.5\} \\
\texttt{batchnorm} & \{True, False\} \\
\texttt{batch\_size} & \{32\} \\
\texttt{lr} & \{0.01, 0.001\} \\
\texttt{optimizer} & \{adam\} \\
\hline
\end{tabularx}
\caption{Neural Networks hyperparameter search space for the first search. While using the \texttt{ff\_tfidf} network, if \texttt{batchnorm} is set to \texttt{True}, the \texttt{dropout} hyperparameter is set to $0$. When using the \texttt{lstm\_embeddings} and \texttt{lstm\_glove} networks, the \texttt{batchnorm} hyperparameter is set to \texttt{False}.}
\label{tab:hyperparameters_neural_network_0}

\end{table}


\begin{table}[H]
\centering
\begin{tabularx}{0.48\textwidth}{|l|X|}
\hline
Hyperparameter & Search Space \\
\hline
\texttt{criterion} & \{gini, log\_loss\} \\
\texttt{splitter} & \{best\} \\
\texttt{max\_depth} & \{1000, None\} \\
\texttt{min\_impurity\_decrease} & \{1e-05, 1e-06, 1e-07\} \\
\texttt{class\_weight} & \{balanced\} \\
\hline
\end{tabularx}
\caption{Decision Trees hyperparameter search space for the second search.}
\label{tab:hyperparameters_decision_tree_1}

\end{table}


\begin{table}[H]
\centering
\begin{tabularx}{0.48\textwidth}{|l|X|}
\hline
Hyperparameter & Search Space \\
\hline
\texttt{n\_estimators} & \{100, 125, 150\} \\
\texttt{criterion} & \{gini, log\_loss\} \\
\texttt{max\_depth} & \{1000, None\} \\
\texttt{min\_impurity\_decrease} & \{1e-05, 1e-06, 1e-07\} \\
\texttt{n\_jobs} & \{-1\} \\
\texttt{class\_weight} & \{balanced\} \\
\hline
\end{tabularx}
\caption{Random Forest hyperparameter search space for the second search.}
\label{tab:hyperparameters_random_forest_1}

\end{table}


\begin{table}[H]
\centering
\begin{tabularx}{0.48\textwidth}{|l|X|}
\hline
Hyperparameter & Search Space \\
\hline
\texttt{network} & \{ff\_tfidf, lstm\_glove\} \\
\texttt{base\_size} & \{16, 32\} \\
\texttt{depth} & \{1, 2, 3\} \\
\texttt{epochs} & \{20\} \\
\texttt{patience} & \{2\} \\
\texttt{dropout} & \{0.5\} \\
\texttt{batchnorm} & \{False\} \\
\texttt{batch\_size} & \{32\} \\
\texttt{lr} & \{0.001\} \\
\texttt{optimizer} & \{adam\} \\
\hline
\end{tabularx}
\caption{Neural Networks hyperparameter search space for the second search. The same rules described in \autoref{tab:hyperparameters_neural_network_0} apply.}
\label{tab:hyperparameters_neural_network_1}

\end{table}


