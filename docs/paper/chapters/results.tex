\section{Results}
\label{sec:results}

The models were compared using their mean
F1 (normalized by class) in the 6-folds 
over the validation set. We can se the 
results for each model in
\autoref{fig:model_selection}.

\end{multicols}
\begin{figure}[H]
    \capstart
    \centering
    \includegraphics[width=\textwidth]{images/model_selection.png}
    \caption{F1 distribution of each model configuration tested during the model selection process.}
    \label{fig:model_selection}
\end{figure}

\begin{figure}[H]
    \capstart
    \centering
    \includegraphics[width=\textwidth]{images/f1_distribution.png}
    \caption{F1 distribution of each model category.}
    \label{fig:f1_distribution}
\end{figure}
\begin{multicols}{2}

Among the models tested in this work, 
the best one was the \textbf{neural network} 
followed by the \textbf{random forest} and then 
the \textbf{decision treee}.
The best models for each kind are shown in
\autoref{tab:best_model_per_kind} and their
respective hyperparameters are shown in
\autoref{tab:hyperparameters_best_neural_network},
\autoref{tab:hyperparameters_best_random_forest} and
\autoref{tab:hyperparameters_best_decision_tree}.

\input{tables/best_model_per_kind}

\input{tables/hyperparameters_best_neural_network}
\input{tables/hyperparameters_best_random_forest}
\input{tables/hyperparameters_best_decision_tree}

The best model was selected based on 
the average F1 obtained in the
cross-validation. The F1 distribution
of this model was compared to the other
models with a Wilcoxon signed-rank test,
\input{dynamic/similar_models}

As we can see in 
\autoref{fig:f1_distribution}, the random
forest is the most susceptible to
hyperparameter changes, as it has the widest
F1 distribution, while the decision tree
is the least susceptible but still 
the worse performing, with the smallest
F1 distribution. The neural network
is in the middle, with a distribution
skewed to higher F1 values.

\input{tables/best_model_metrics_test}

\begin{figure}[H]
    \capstart
    \centering
    \includegraphics[width=0.48\textwidth]{images/best_model_confusion_matrix_test.png}
    \caption{Confusion matrix of the best model.}
    \label{fig:confusion_matrix_test}
\end{figure}

The best model was also tested on a subset of the
test set, composed of only the instances that
were taken from the CrowdFlower dataset. The
results can be compared to the results obtained
by those of the paper from Batbaatar et al.
\cite{emotion_recognition_from_text}. The metrics
obtained are shown in 
\autoref{tab:best_model_metrics_crowdflower_test}.
We can see that the model has a worse performance
on this subset compared to the model from the
paper (51.1\% of F1), this can be 
explained by the much more complex model used
by the authors: a deep LSTM and CNN network with 
different kinds of embedding that extract both 
semantic information and sentiment information 
from the text.

\input{tables/best_model_metrics_crowdflower_test}
