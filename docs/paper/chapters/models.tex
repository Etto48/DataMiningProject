\section{Models}
\label{sec:models}
Three different categories of models were
considered:
\begin{itemize}
    \item \textit{Decision Trees}
    \item \textit{Random Forests}
    \item \textit{Neural Networks}
    \begin{itemize}
        \item \textit{Feedforward Networks}
        \item \textit{LSTM Networks}
        \item \textit{Transformer Networks}
    \end{itemize}
\end{itemize}
\textbf{Decision trees} and \textbf{random forests} 
both use the BoW preprocessing.
feedforward \textbf{neural networks} use the BoW
preprocessing with \textbf{TFIDF} encoding.
LSTM Networks and Transformer Networks
use the GE preprocessing.

\subsection{Imbalanced Dataset}
To handle the imbalanced dataset, a class
weight was assigned to each class. The weight
for a class $c$ was calculated as
$W_c=\frac{|D|}{|C||D_c|}$ where $|D|$ is
the size of the dataset, $|C|$ is the number
of classes and $|D_c|$ is the number of
samples in class $c$. This weight was used in
the criterion of the \textbf{decision trees} 
and \textbf{random forests} models, and 
were used for resampling the training dataset
(with replacement) in the 
\textbf{neural networks} models.

\subsection{Model Selection}
To find a good hyperparameter configuration
for each model, a random search was performed.
The search was done with 10 iterations for
each model (\textbf{decision trees}, \textbf{random forests} 
and \textbf{neural networks}). Each configuration was
evaluated with a 6-fold cross-validation on
the training set. Two subsequent random
searches were performed, the second one
narrowing the search space around the best
configuration found in the first search.
The search spaces for each model are shown
in \autoref{tab:hyperparameters_decision_tree_0}, 
\autoref{tab:hyperparameters_random_forest_0} and 
\autoref{tab:hyperparameters_neural_network_0} for the 
first search, and in 
\autoref{tab:hyperparameters_decision_tree_1},
\autoref{tab:hyperparameters_random_forest_1} and
\autoref{tab:hyperparameters_neural_network_1} for the
second search.
Neural Networks used the \textbf{early stopping}
technique to avoid overfitting and speed up the
training process.

\input{tables/hypers}
